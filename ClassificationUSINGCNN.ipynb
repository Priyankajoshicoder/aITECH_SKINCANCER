{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D3AtFrCrxd_z",
        "outputId": "6cc7cc5d-9fcd-4383-9561-cfd83ef03a1f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import concurrent.futures\n",
        "import gc\n",
        "import time\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "    print(f\"Found {len(physical_devices)} GPU(s), memory growth enabled\")\n",
        "except Exception as e:\n",
        "    print(f\"Memory growth setting error: {e}\")\n",
        "\n",
        "# Check if GPU is available and print device information\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available:\")\n",
        "    for gpu in tf.config.list_physical_devices('GPU'):\n",
        "        print(f\"  {gpu}\")\n",
        "    print(f\"TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "else:\n",
        "    print(\"No GPU found! Running on CPU.\")\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 64  # Increased batch size for better GPU utilization\n",
        "EPOCHS = 20\n",
        "CLASSES = ['basal cell carcinoma', 'dermatofibroma', 'melanoma', 'squamous cell carcinoma']\n",
        "CLASS_MAPPING = {cls: i for i, cls in enumerate(CLASSES)}\n",
        "AUGMENTATION_FACTOR = 20  # Create 20 versions of each dark skin image\n",
        "\n",
        "# Function to create DataFrame from image directory\n",
        "def create_dataframe(data_dir):\n",
        "    data = []\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        if class_name not in CLASSES:\n",
        "            continue\n",
        "\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            if os.path.isfile(img_path) and img_path.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                data.append({\n",
        "                    \"image_path\": img_path,\n",
        "                    \"label\": class_name,\n",
        "                    \"class_id\": CLASS_MAPPING[class_name],\n",
        "                    \"is_augmented\": False,\n",
        "                    \"source\": os.path.basename(data_dir)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"Created DataFrame with {len(df)} images from {data_dir}\")\n",
        "    return df\n",
        "\n",
        "# Function to remove hair from skin lesion images\n",
        "def remove_hair(image):\n",
        "    # Convert to grayscale\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        grayscale = image\n",
        "\n",
        "    # Create kernel for morphology operation\n",
        "    kernel = cv2.getStructuringElement(1, (17, 17))\n",
        "\n",
        "    # Apply blackhat filter\n",
        "    blackhat = cv2.morphologyEx(grayscale, cv2.MORPH_BLACKHAT, kernel)\n",
        "\n",
        "    # Apply threshold\n",
        "    _, mask = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Inpaint\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        result = cv2.inpaint(image, mask, 1, cv2.INPAINT_TELEA)\n",
        "    else:\n",
        "        result = cv2.inpaint(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), mask, 1, cv2.INPAINT_TELEA)\n",
        "        result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Efficient image loading with tf.data\n",
        "def preprocess_image(image_path, label):\n",
        "    \"\"\"Load and preprocess an image using TensorFlow operations\"\"\"\n",
        "    # Read the image file\n",
        "    img = tf.io.read_file(image_path)\n",
        "\n",
        "    # Decode the image\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "\n",
        "    # Resize the image\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "\n",
        "    # Normalize to [0,1] and EXPLICITLY set dtype to float32 for consistency\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "    # Also make sure the label has consistent dtype\n",
        "    label = tf.cast(label, tf.float32)\n",
        "\n",
        "    return img, label\n",
        "\n",
        "# Create tf.data.Dataset for efficient loading\n",
        "def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, is_training=True):\n",
        "    \"\"\"Create an optimized tf.data pipeline for image loading\"\"\"\n",
        "    # Convert labels to float32 for consistency\n",
        "    labels = labels.astype(np.float32)\n",
        "\n",
        "    # Create dataset from tensors\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    # Map preprocessing function\n",
        "    dataset = dataset.map(\n",
        "        preprocess_image,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE  # Use multiple CPU cores\n",
        "    )\n",
        "\n",
        "    if is_training:\n",
        "        # Shuffle and repeat for training\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "        # Add data augmentation for training\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (data_augmentation(x, training=True), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "    # Batch the data\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Prefetch for better performance\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# TensorFlow-based data augmentation (runs on GPU)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "])\n",
        "\n",
        "# Process dark skin images (in batches to save memory)\n",
        "def process_images_in_batches(image_paths, batch_size=50):\n",
        "    \"\"\"Process images in batches to avoid memory issues\"\"\"\n",
        "    processed_images = []\n",
        "    total_batches = len(image_paths) // batch_size + (1 if len(image_paths) % batch_size > 0 else 0)\n",
        "\n",
        "    for i in range(total_batches):\n",
        "        batch_paths = image_paths[i * batch_size:(i + 1) * batch_size]\n",
        "        batch_images = []\n",
        "\n",
        "        for path in batch_paths:\n",
        "            try:\n",
        "                # Load image\n",
        "                img = np.array(Image.open(path).convert('RGB'))\n",
        "\n",
        "                # Remove hair\n",
        "                img_no_hair = remove_hair(img)\n",
        "\n",
        "                # Resize\n",
        "                resized_img = cv2.resize(img_no_hair, IMAGE_SIZE)\n",
        "\n",
        "                batch_images.append(resized_img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {path}: {e}\")\n",
        "                # Return a blank image in case of error\n",
        "                batch_images.append(np.zeros((IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype=np.uint8))\n",
        "\n",
        "        processed_images.extend(batch_images)\n",
        "\n",
        "        # Force garbage collection after each batch\n",
        "        gc.collect()\n",
        "\n",
        "    return processed_images\n",
        "\n",
        "# Enhanced function to create heavily augmented versions of dark skin images\n",
        "def create_augmented_dark_skin_dataset(dark_skin_df, multiplier=AUGMENTATION_FACTOR):\n",
        "    \"\"\"\n",
        "    Create heavily augmented versions of dark skin images to increase effective dataset size\n",
        "\n",
        "    Parameters:\n",
        "    dark_skin_df: DataFrame with dark skin images\n",
        "    multiplier: How many augmented versions to create per original image\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with original and augmented images\n",
        "    \"\"\"\n",
        "    print(f\"Creating {multiplier}x augmented versions of {len(dark_skin_df)} dark skin images...\")\n",
        "\n",
        "    # Create a stronger augmentation pipeline specifically for dark skin\n",
        "    dark_skin_datagen = ImageDataGenerator(\n",
        "        # Geometric transformations\n",
        "        rotation_range=45,\n",
        "        width_shift_range=0.3,\n",
        "        height_shift_range=0.3,\n",
        "        zoom_range=[0.8, 1.2],\n",
        "        horizontal_flip=True,\n",
        "\n",
        "        # Color/intensity transformations (critical for skin tone)\n",
        "        brightness_range=[0.6, 1.4],\n",
        "        channel_shift_range=0.4,\n",
        "        fill_mode='reflect',\n",
        "\n",
        "        # Shear for shape variation\n",
        "        shear_range=0.2\n",
        "    )\n",
        "\n",
        "    # Create new dataframe to store augmented images\n",
        "    augmented_data = []\n",
        "\n",
        "    # Process each class separately to ensure balanced augmentation\n",
        "    for class_name in dark_skin_df['label'].unique():\n",
        "        class_df = dark_skin_df[dark_skin_df['label'] == class_name]\n",
        "\n",
        "        print(f\"Augmenting {len(class_df)} images from class: {class_name}\")\n",
        "\n",
        "        # Process in smaller batches to avoid memory issues\n",
        "        for i in range(0, len(class_df), 3):  # Even smaller batch size (3)\n",
        "            batch_df = class_df.iloc[i:i+3]\n",
        "\n",
        "            # Create augmentations for this batch\n",
        "            for _, row in batch_df.iterrows():\n",
        "                # Add original image\n",
        "                augmented_data.append(row.to_dict())\n",
        "\n",
        "                # Create image batch for augmentation\n",
        "                img = row['image'].reshape(1, *row['image'].shape)\n",
        "\n",
        "                # Generate augmented versions in smaller chunks to save memory\n",
        "                for j in range(0, multiplier-1, 5):\n",
        "                    # Create 5 augmented versions at a time (or fewer for the last chunk)\n",
        "                    chunk_size = min(5, multiplier-1-j)\n",
        "                    aug_generator = dark_skin_datagen.flow(img, batch_size=1)\n",
        "\n",
        "                    for k in range(chunk_size):\n",
        "                        aug_img = next(aug_generator)[0].astype('uint8')\n",
        "\n",
        "                        # Create new row with augmented image\n",
        "                        new_row = row.to_dict()\n",
        "                        new_row['image'] = aug_img\n",
        "                        new_row['is_augmented'] = True\n",
        "                        new_row['aug_id'] = j + k + 1\n",
        "\n",
        "                        # Add to dataset\n",
        "                        augmented_data.append(new_row)\n",
        "\n",
        "                    # Clear memory after each chunk\n",
        "                    gc.collect()\n",
        "\n",
        "            # Clear memory after each batch\n",
        "            gc.collect()\n",
        "\n",
        "    # Convert to dataframe\n",
        "    result_df = pd.DataFrame(augmented_data)\n",
        "\n",
        "    # Print augmentation summary\n",
        "    print(\"\\nAugmentation Summary:\")\n",
        "    class_counts = result_df.groupby('label').size()\n",
        "    original_counts = dark_skin_df.groupby('label').size()\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Class':<30} {'Original':<10} {'Total':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for cls in CLASSES:\n",
        "        if cls in original_counts:\n",
        "            orig = original_counts[cls]\n",
        "            total = class_counts.get(cls, 0)\n",
        "            print(f\"{cls:<30} {orig:<10} {total:<10}\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'TOTAL':<30} {len(dark_skin_df):<10} {len(result_df):<10}\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Function to create a DenseNet model with mixed precision\n",
        "def create_model(input_shape=(128, 128, 3), num_classes=4):\n",
        "    # Enable mixed precision just for the model\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "    # Place the model on GPU explicitly\n",
        "    with tf.device('/GPU:0'):\n",
        "        # Create base DenseNet model\n",
        "        base_model = DenseNet201(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=input_shape,\n",
        "            pooling='avg'\n",
        "        )\n",
        "\n",
        "        # Freeze early layers (transfer learning)\n",
        "        for layer in base_model.layers[:-30]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Add classification layers\n",
        "        x = base_model.output\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "        # Use SGD optimizer\n",
        "        opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=opt,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to visualize training progress\n",
        "def plot_training_history(history, title=\"Model Training\", output_dir=\".\"):\n",
        "    \"\"\"Plot the training history to monitor for GPU performance issues\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(output_dir, f\"{title.replace(' ', '_').lower()}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "    print(f\"Training history plot saved to {save_path}\")\n",
        "\n",
        "# Function to create and save confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Create and save a confusion matrix visualization\n",
        "\n",
        "    Parameters:\n",
        "    y_true: True labels (class indices)\n",
        "    y_pred: Predicted labels (class indices)\n",
        "    class_names: List of class names for axis labels\n",
        "    title: Title for the plot\n",
        "    output_dir: Directory to save the output\n",
        "    \"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Create a nice visualization\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "\n",
        "    # Add titles and formatting\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.grid(False)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    save_path = os.path.join(output_dir, f\"{title.replace(' ', '_').lower()}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "    print(f\"Confusion matrix saved to {save_path}\")\n",
        "\n",
        "    return cm\n",
        "\n",
        "# Function to create and save classification report\n",
        "def save_classification_report(y_true, y_pred, class_names, title=\"Classification Report\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Generate and save a classification report\n",
        "\n",
        "    Parameters:\n",
        "    y_true: True labels (class indices)\n",
        "    y_pred: Predicted labels (class indices)\n",
        "    class_names: List of class names\n",
        "    title: Title for the report\n",
        "    output_dir: Directory to save the output\n",
        "    \"\"\"\n",
        "    # Generate the classification report\n",
        "    report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=class_names,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    # Convert to DataFrame for better formatting\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Print the report\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    # Save the report to CSV\n",
        "    save_path = os.path.join(output_dir, f\"{title.replace(' ', '_').lower()}.csv\")\n",
        "    report_df.to_csv(save_path)\n",
        "    print(f\"Classification report saved to {save_path}\")\n",
        "\n",
        "    return report_df\n",
        "\n",
        "# Function to plot model comparison\n",
        "def plot_model_comparison(baseline_results, combined_results, class_names, output_dir=\".\"):\n",
        "    \"\"\"Plot a comparison of model performances\"\"\"\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Plot overall accuracy comparison\n",
        "    plt.subplot(1, 2, 1)\n",
        "    models = ['Baseline (ISIC only)', 'Combined (ISIC + Dark Skin)']\n",
        "    accuracies = [\n",
        "        baseline_results['overall']['accuracy'],\n",
        "        combined_results['overall']['accuracy']\n",
        "    ]\n",
        "    colors = ['#3498db', '#2ecc71']\n",
        "\n",
        "    plt.bar(models, accuracies, color=colors)\n",
        "    plt.title('Overall Accuracy on Dark Skin Test Images')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Annotate exact values\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot per-class accuracy comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Set up data\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.35\n",
        "\n",
        "    baseline_class_acc = [baseline_results['per_class'][cls]['accuracy'] for cls in class_names]\n",
        "    combined_class_acc = [combined_results['per_class'][cls]['accuracy'] for cls in class_names]\n",
        "\n",
        "    # Plot bars\n",
        "    plt.bar(x - width/2, baseline_class_acc, width, label='Baseline Model', color='#3498db')\n",
        "    plt.bar(x + width/2, combined_class_acc, width, label='Combined Model', color='#2ecc71')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.title('Per-Class Accuracy on Dark Skin Test Images')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(x, [c.split()[-1] for c in class_names], rotation=45, ha='right')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(output_dir, \"model_comparison.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "    print(f\"Model comparison plot saved to {save_path}\")\n",
        "\n",
        "# Monitor memory usage\n",
        "def process_memory_usage():\n",
        "    \"\"\"Return memory usage in GB\"\"\"\n",
        "    import psutil\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    return memory_info.rss / (1024 ** 3)  # Convert bytes to GB\n",
        "\n",
        "# Main function with memory-optimized approach\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Define paths\n",
        "    isic_train_dir = 'C:\\\\Users\\\\Priyanka Joshi\\\\Downloads\\\\ai_tech_project\\\\ISIC\\\\Train'\n",
        "    isic_test_dir = 'C:\\\\Users\\\\Priyanka Joshi\\\\Downloads\\\\ai_tech_project\\\\ISIC\\\\Test'\n",
        "    dark_skin_dir = 'C:\\\\Users\\\\Priyanka Joshi\\\\Downloads\\\\ai_tech_project\\\\darkskin'\n",
        "    output_dir = 'C:\\\\Users\\\\Priyanka Joshi\\Downloads\\\\ai_tech_project\\\\output'\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Step 1: Create dataframes with paths only (no images yet)\n",
        "    print(\"Creating dataframes from directories...\")\n",
        "    isic_train_df = create_dataframe(isic_train_dir)\n",
        "    isic_test_df = create_dataframe(isic_test_dir)\n",
        "    dark_skin_df = create_dataframe(dark_skin_dir)\n",
        "\n",
        "    # Step 2: Process dark skin images (smaller dataset)\n",
        "    print(\"Processing dark skin images...\")\n",
        "    dark_skin_df['image'] = process_images_in_batches(dark_skin_df['image_path'].tolist())\n",
        "\n",
        "    # Step 3: Split dark skin data BEFORE augmentation to prevent data leakage\n",
        "    dark_skin_train, dark_skin_test = train_test_split(\n",
        "        dark_skin_df,\n",
        "        test_size=0.4,\n",
        "        stratify=dark_skin_df['label'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Dark skin split (before augmentation): {len(dark_skin_train)} training, {len(dark_skin_test)} testing\")\n",
        "\n",
        "    # Memory check point - print current memory usage\n",
        "    try:\n",
        "        print(f\"Memory usage checkpoint 1: {process_memory_usage():.2f} GB\")\n",
        "    except:\n",
        "        print(\"Memory monitoring not available\")\n",
        "\n",
        "    # Step 4: Augment dark skin training images\n",
        "    print(\"\\nAugmenting dark skin training images...\")\n",
        "    augmented_dark_skin_train = create_augmented_dark_skin_dataset(dark_skin_train)\n",
        "\n",
        "    # Memory check point\n",
        "    try:\n",
        "        print(f\"Memory usage checkpoint 2: {process_memory_usage():.2f} GB\")\n",
        "    except:\n",
        "        print(\"Memory monitoring not available\")\n",
        "\n",
        "    # Step 5: Create TensorFlow datasets for ISIC data (efficient loading)\n",
        "    print(\"\\nCreating TensorFlow datasets for efficient loading...\")\n",
        "\n",
        "    # For ISIC train - prepare labels first\n",
        "    isic_train_labels = tf.keras.utils.to_categorical(\n",
        "        isic_train_df['class_id'].values,\n",
        "        num_classes=len(CLASSES)\n",
        "    )\n",
        "\n",
        "    # Create train dataset that loads and processes images on-the-fly\n",
        "    train_dataset = create_dataset(\n",
        "        isic_train_df['image_path'].values,\n",
        "        isic_train_labels,\n",
        "        is_training=True\n",
        "    )\n",
        "\n",
        "    # For ISIC test data\n",
        "    isic_test_labels = tf.keras.utils.to_categorical(\n",
        "        isic_test_df['class_id'].values,\n",
        "        num_classes=len(CLASSES)\n",
        "    )\n",
        "\n",
        "    test_dataset = create_dataset(\n",
        "        isic_test_df['image_path'].values,\n",
        "        isic_test_labels,\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    # Step 6: Prepare dark skin test data\n",
        "    # Ensure consistent float32 dtype\n",
        "    X_dark_test = np.stack(dark_skin_test['image'].values).astype(np.float32) / 255.0\n",
        "    y_dark_test = tf.keras.utils.to_categorical(dark_skin_test['class_id'], num_classes=len(CLASSES)).astype(np.float32)\n",
        "    y_dark_test_labels = dark_skin_test['class_id'].values\n",
        "\n",
        "    # Step 7: Prepare augmented dark skin training data\n",
        "    # Ensure consistent float32 dtype\n",
        "    X_dark_train_aug = np.stack(augmented_dark_skin_train['image'].values).astype(np.float32) / 255.0\n",
        "    y_dark_train_aug = tf.keras.utils.to_categorical(\n",
        "        augmented_dark_skin_train['class_id'],\n",
        "        num_classes=len(CLASSES)\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    # Create a tf.data.Dataset for dark skin data\n",
        "    dark_skin_dataset = tf.data.Dataset.from_tensor_slices((X_dark_train_aug, y_dark_train_aug))\n",
        "    dark_skin_dataset = dark_skin_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Clear RAM by removing dataframes we no longer need\n",
        "    del dark_skin_df\n",
        "    del dark_skin_train\n",
        "    del augmented_dark_skin_train\n",
        "    gc.collect()\n",
        "\n",
        "    # Memory check point\n",
        "    try:\n",
        "        print(f\"Memory usage checkpoint 3: {process_memory_usage():.2f} GB\")\n",
        "    except:\n",
        "        print(\"Memory monitoring not available\")\n",
        "\n",
        "    # Print class distribution in dark skin test set\n",
        "    print(\"\\nDark skin test set class distribution:\")\n",
        "    for cls, idx in CLASS_MAPPING.items():\n",
        "        count = np.sum(y_dark_test_labels == idx)\n",
        "        print(f\"  {cls}: {count} images\")\n",
        "\n",
        "    # Step 8: Create callbacks for training\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            patience=2,\n",
        "            factor=0.5,\n",
        "            min_lr=0.00001,\n",
        "            verbose=1\n",
        "        ),\n",
        "        # Add TensorBoard logging\n",
        "        tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=os.path.join(output_dir, 'logs'),\n",
        "            histogram_freq=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Step 9: TRAIN BASELINE MODEL (ISIC data only)\n",
        "    print(\"\\nTraining baseline model on ISIC data only...\")\n",
        "    # Reset global policy before creating each model\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    baseline_model = create_model()\n",
        "\n",
        "    # Print model summary\n",
        "    baseline_model.summary()\n",
        "\n",
        "    # Train baseline model\n",
        "    baseline_history = baseline_model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(baseline_history, \"Baseline Model\", output_dir)\n",
        "\n",
        "    # Step 10: TRAIN COMBINED MODEL (using separate approach instead of concatenate)\n",
        "    print(\"\\nTraining combined model on ISIC + augmented dark skin data...\")\n",
        "    # Reset global policy before creating each model\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    combined_model = create_model()\n",
        "\n",
        "    # Instead of concatenating, use a custom training loop with both datasets\n",
        "    # First train on ISIC data for a few epochs\n",
        "    print(\"Phase 1: Training on ISIC data...\")\n",
        "    combined_model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=EPOCHS // 2,  # Train for half the epochs\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Then fine-tune on dark skin data\n",
        "    print(\"Phase 2: Fine-tuning on dark skin data...\")\n",
        "    combined_history = combined_model.fit(\n",
        "        dark_skin_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=EPOCHS // 2,  # Train for the remaining epochs\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(combined_history, \"Combined Model\", output_dir)\n",
        "\n",
        "    # Step 11: EVALUATE MODELS ON DARK SKIN TEST DATA\n",
        "    print(\"\\nEvaluating both models on dark skin test data...\")\n",
        "\n",
        "    # Create a tf.data.Dataset for dark skin test for more efficient evaluation\n",
        "    dark_test_dataset = tf.data.Dataset.from_tensor_slices((X_dark_test, y_dark_test))\n",
        "    dark_test_dataset = dark_test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Evaluate baseline model on dark skin test data\n",
        "    baseline_dark_loss, baseline_dark_acc = baseline_model.evaluate(dark_test_dataset)\n",
        "    baseline_preds = baseline_model.predict(dark_test_dataset)\n",
        "    baseline_pred_classes = np.argmax(baseline_preds, axis=1)\n",
        "\n",
        "    # Evaluate combined model on dark skin test data\n",
        "    combined_dark_loss, combined_dark_acc = combined_model.evaluate(dark_test_dataset)\n",
        "    combined_preds = combined_model.predict(dark_test_dataset)\n",
        "    combined_pred_classes = np.argmax(combined_preds, axis=1)\n",
        "\n",
        "    # Step 12: Generate and save confusion matrices\n",
        "    baseline_cm = plot_confusion_matrix(\n",
        "        y_dark_test_labels,\n",
        "        baseline_pred_classes,\n",
        "        CLASSES,\n",
        "        title=\"Baseline Model Confusion Matrix\",\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    combined_cm = plot_confusion_matrix(\n",
        "        y_dark_test_labels,\n",
        "        combined_pred_classes,\n",
        "        CLASSES,\n",
        "        title=\"Combined Model Confusion Matrix\",\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    # Step 13: Generate and save classification reports\n",
        "    baseline_report = save_classification_report(\n",
        "        y_dark_test_labels,\n",
        "        baseline_pred_classes,\n",
        "        CLASSES,\n",
        "        title=\"Baseline Model Classification Report\",\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    combined_report = save_classification_report(\n",
        "        y_dark_test_labels,\n",
        "        combined_pred_classes,\n",
        "        CLASSES,\n",
        "        title=\"Combined Model Classification Report\",\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    # Step 14: Store and analyze results\n",
        "    # Print initial results\n",
        "    print(f\"Baseline model accuracy on dark skin test data: {baseline_dark_acc:.4f}\")\n",
        "    print(f\"Combined model accuracy on dark skin test data: {combined_dark_acc:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    baseline_results = {\n",
        "        'overall': {'accuracy': baseline_dark_acc, 'loss': baseline_dark_loss},\n",
        "        'per_class': {cls: {'correct': 0, 'total': 0} for cls in CLASSES}\n",
        "    }\n",
        "\n",
        "    combined_results = {\n",
        "        'overall': {'accuracy': combined_dark_acc, 'loss': combined_dark_loss},\n",
        "        'per_class': {cls: {'correct': 0, 'total': 0} for cls in CLASSES}\n",
        "    }\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    for i, true_label in enumerate(y_dark_test_labels):\n",
        "        true_class = CLASSES[true_label]\n",
        "        baseline_correct = (baseline_pred_classes[i] == true_label)\n",
        "        combined_correct = (combined_pred_classes[i] == true_label)\n",
        "\n",
        "        # Update baseline results\n",
        "        baseline_results['per_class'][true_class]['total'] += 1\n",
        "        baseline_results['per_class'][true_class]['correct'] += int(baseline_correct)\n",
        "\n",
        "        # Update combined results\n",
        "        combined_results['per_class'][true_class]['total'] += 1\n",
        "        combined_results['per_class'][true_class]['correct'] += int(combined_correct)\n",
        "\n",
        "    # Calculate per-class accuracies\n",
        "    for cls in CLASSES:\n",
        "        for results in [baseline_results, combined_results]:\n",
        "            if results['per_class'][cls]['total'] > 0:\n",
        "                results['per_class'][cls]['accuracy'] = results['per_class'][cls]['correct'] / results['per_class'][cls]['total']\n",
        "            else:\n",
        "                results['per_class'][cls]['accuracy'] = 0\n",
        "\n",
        "    # Plot model comparison\n",
        "    plot_model_comparison(baseline_results, combined_results, CLASSES, output_dir)\n",
        "\n",
        "    # Calculate improvement\n",
        "    improvement = combined_results['overall']['accuracy'] - baseline_results['overall']['accuracy']\n",
        "    print(f\"\\nOverall improvement from adding dark skin data: {improvement:.4f} \" +\n",
        "          f\"({improvement*100:.1f}% {'increase' if improvement >= 0 else 'decrease'})\")\n",
        "\n",
        "    # Save the models\n",
        "    baseline_model.save(os.path.join(output_dir, 'baseline_model.h5'))\n",
        "    combined_model.save(os.path.join(output_dir, 'combined_model.h5'))\n",
        "    print(\"Models saved to output directory\")\n",
        "\n",
        "    # Generate summary table of results\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Metric': ['Overall Accuracy', 'Loss'] + [f\"{cls} Accuracy\" for cls in CLASSES],\n",
        "        'Baseline Model': [baseline_results['overall']['accuracy'], baseline_results['overall']['loss']] +\n",
        "                          [baseline_results['per_class'][cls]['accuracy'] for cls in CLASSES],\n",
        "        'Combined Model': [combined_results['overall']['accuracy'], combined_results['overall']['loss']] +\n",
        "                          [combined_results['per_class'][cls]['accuracy'] for cls in CLASSES],\n",
        "        'Improvement': [improvement, baseline_results['overall']['loss'] - combined_results['overall']['loss']] +\n",
        "                       [combined_results['per_class'][cls]['accuracy'] - baseline_results['per_class'][cls]['accuracy'] for cls in CLASSES]\n",
        "    })\n",
        "\n",
        "    # Save summary table\n",
        "    summary_path = os.path.join(output_dir, 'model_comparison_summary.csv')\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"Summary table saved to {summary_path}\")\n",
        "\n",
        "    # Print summary table\n",
        "    print(\"\\nModel Comparison Summary:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Calculate total runtime\n",
        "    total_time = time.time() - start_time\n",
        "    hours, remainder = divmod(total_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"\\nTotal runtime: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Add psutil for memory monitoring\n",
        "        import psutil\n",
        "    except ImportError:\n",
        "        print(\"Installing psutil for memory monitoring...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", \"psutil\"])\n",
        "        import psutil\n",
        "\n",
        "    # Run the main function\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
